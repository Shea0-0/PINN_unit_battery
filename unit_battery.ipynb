{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMDKq4qwpXn3LdjuhamMZBa"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":0},"id":"97niiMgoyaRT","executionInfo":{"status":"error","timestamp":1706692624436,"user_tz":-480,"elapsed":5,"user":{"displayName":"Shea","userId":"17719700703267347051"}},"outputId":"23ebf8df-6607-4138-f69e-6360fc9ddef9"},"outputs":[{"output_type":"error","ename":"FileNotFoundError","evalue":"[Errno 2] No such file or directory: './CP2.parquet'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-2-ab2107d83868>\u001b[0m in \u001b[0;36m<cell line: 15>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmin_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_val\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mmin_val\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mcp_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_parquet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./CP2.parquet'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0mic_bc_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_parquet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./IC_BC2.parquet'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0mall_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcp_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mic_bc_data\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parquet.py\u001b[0m in \u001b[0;36mread_parquet\u001b[0;34m(path, engine, columns, storage_options, use_nullable_dtypes, **kwargs)\u001b[0m\n\u001b[1;32m    501\u001b[0m     \u001b[0mimpl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    502\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 503\u001b[0;31m     return impl.read(\n\u001b[0m\u001b[1;32m    504\u001b[0m         \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    505\u001b[0m         \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parquet.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, path, columns, use_nullable_dtypes, storage_options, **kwargs)\u001b[0m\n\u001b[1;32m    242\u001b[0m             \u001b[0mto_pandas_kwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"split_blocks\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m  \u001b[0;31m# type: ignore[assignment]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 244\u001b[0;31m         path_or_handle, handles, kwargs[\"filesystem\"] = _get_path_or_handle(\n\u001b[0m\u001b[1;32m    245\u001b[0m             \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    246\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"filesystem\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parquet.py\u001b[0m in \u001b[0;36m_get_path_or_handle\u001b[0;34m(path, fs, storage_options, mode, is_dir)\u001b[0m\n\u001b[1;32m    100\u001b[0m         \u001b[0;31m# fsspec resources can also point to directories\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m         \u001b[0;31m# this branch is used for example when reading from non-fsspec URLs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m         handles = get_handle(\n\u001b[0m\u001b[1;32m    103\u001b[0m             \u001b[0mpath_or_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_text\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstorage_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m         )\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    863\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    864\u001b[0m             \u001b[0;31m# Binary mode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 865\u001b[0;31m             \u001b[0mhandle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    866\u001b[0m         \u001b[0mhandles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    867\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './CP2.parquet'"]}],"source":["import torch\n","import torch.nn as nn\n","import numpy as np\n","import os\n","import matplotlib.pyplot as plt\n","import pandas as pd\n","import plotly.graph_objects as go\n","from plotly.subplots import make_subplots\n","\n","def min_max(data):\n","    min_val = torch.min(data)\n","    max_val = torch.max(data)\n","    return min_val, max_val, max_val-min_val\n","\n","cp_data = pd.read_parquet('./CP2.parquet')\n","ic_bc_data = pd.read_parquet('./IC_BC2.parquet')\n","all_data = pd.concat([cp_data, ic_bc_data])\n","\n","all_position = all_data[['X', 'Y', 'Z']].drop_duplicates()\n","cp_position = cp_data[['X', 'Y', 'Z']].drop_duplicates()\n","\n","# Set default dtype to float32\n","torch.set_default_dtype(torch.float)\n","# PyTorch random number generator\n","torch.manual_seed(1234)\n","# Random number generators in other libraries\n","np.random.seed(1234)\n","# Device configuration\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","X_test = torch.tensor(all_data[['X']].values, dtype=torch.float32)\n","Y_test = torch.tensor(all_data[['Y']].values, dtype=torch.float32)\n","Z_test = torch.tensor(all_data[['Z']].values, dtype=torch.float32)\n","time_test = torch.tensor(all_data[['time']].values, dtype=torch.float32)\n","Temp_test = torch.tensor(all_data[['Temp']].values, dtype=torch.float32)\n","total_heat_test = torch.tensor(all_data[['total_heat']].values, dtype=torch.float32)\n","\n","X_CP = torch.tensor(cp_data[['X']].values, dtype=torch.float32)\n","Y_CP = torch.tensor(cp_data[['Y']].values, dtype=torch.float32)\n","Z_CP = torch.tensor(cp_data[['Z']].values, dtype=torch.float32)\n","time_CP = torch.tensor(cp_data[['time']].values, dtype=torch.float32)\n","Temp_CP = torch.tensor(cp_data[['Temp']].values, dtype=torch.float32)\n","total_heat_CP = torch.tensor(cp_data[['total_heat']].values, dtype=torch.float32)\n","\n","# 此处X_IB指cell的IC BC,ic_bc_data包含了所有的tab和cell的ICBC，而CP为cell非ICBC的部分\n","X_IB = torch.tensor(ic_bc_data[ic_bc_data['is_tab'] == 0][['X']].values, dtype=torch.float32)\n","Y_IB = torch.tensor(ic_bc_data[ic_bc_data['is_tab'] == 0][['Y']].values, dtype=torch.float32)\n","Z_IB = torch.tensor(ic_bc_data[ic_bc_data['is_tab'] == 0][['Z']].values, dtype=torch.float32)\n","time_IB = torch.tensor(ic_bc_data[ic_bc_data['is_tab'] == 0][['time']].values, dtype=torch.float32)\n","Temp_IB = torch.tensor(ic_bc_data[ic_bc_data['is_tab'] == 0][['Temp']].values, dtype=torch.float32)\n","total_heat_IB = torch.tensor(ic_bc_data[ic_bc_data['is_tab'] == 0][['total_heat']].values, dtype=torch.float32)\n","\n","X_tab = torch.tensor(ic_bc_data[ic_bc_data['is_tab'] == 1][['X']].values, dtype=torch.float32)\n","Y_tab = torch.tensor(ic_bc_data[ic_bc_data['is_tab'] == 1][['Y']].values, dtype=torch.float32)\n","Z_tab = torch.tensor(ic_bc_data[ic_bc_data['is_tab'] == 1][['Z']].values, dtype=torch.float32)\n","time_tab = torch.tensor(ic_bc_data[ic_bc_data['is_tab'] == 1][['time']].values, dtype=torch.float32)\n","Temp_tab = torch.tensor(ic_bc_data[ic_bc_data['is_tab'] == 1][['Temp']].values, dtype=torch.float32)\n","total_heat_tab = torch.tensor(ic_bc_data[ic_bc_data['is_tab'] == 1][['total_heat']].values, dtype=torch.float32)\n","\n","# Nu对应ICBC，Nf对应CP\n","Nu_cell = int(X_IB.shape[0] / 50)\n","Nu_tab = int(X_tab.shape[0] / 50)\n","idx_Nu_cell = np.sort(np.random.choice(X_IB.shape[0], Nu_cell, replace=False))\n","idx_Nu_tab = np.sort(np.random.choice(X_tab.shape[0], Nu_tab, replace=False))\n","X_Nu_cell = X_IB[idx_Nu_cell, :].float()  # Training Points  of x at (IC+BC)\n","X_Nu_tab = X_tab[idx_Nu_tab, :].float()  # Training Points  of x at (IC+BC)\n","time_Nu_cell = time_IB[idx_Nu_cell, :].float()  # Training Points  of t at (IC+BC)\n","time_Nu_tab = time_tab[idx_Nu_tab, :].float()  # Training Points  of t at (IC+BC)\n","Y_Nu_cell = Y_IB[idx_Nu_cell, :].float()  # Training Points  of y at (IC+BC)\n","Y_Nu_tab = Y_tab[idx_Nu_tab, :].float()  # Training Points  of y at (IC+BC)\n","Z_Nu_cell = Z_IB[idx_Nu_cell, :].float()  # Training Points  of y at (IC+BC)\n","Z_Nu_tab = Z_tab[idx_Nu_tab, :].float()  # Training Points  of y at (IC+BC)\n","total_heat_Nu_cell = total_heat_IB[idx_Nu_cell, :].float()  # Training Points  of y at (IC+BC)\n","total_heat_Nu_tab = total_heat_tab[idx_Nu_tab, :].float()  # Training Points  of y at (IC+BC)\n","Temp_Nu_cell = Temp_IB[idx_Nu_cell, :].float()\n","Temp_Nu_tab = Temp_tab[idx_Nu_tab, :].float()\n","\n","time_steps = cp_data['time'].nunique()\n","nodes_cell_num = cp_data[['X', 'Y', 'Z']].drop_duplicates().shape[0]\n","\n","Nf_cell = 3400  # Nf: Number of collocation points\n","idx_Nf_cell = np.sort(np.random.choice(nodes_cell_num*time_steps, Nf_cell, replace=False))\n","X_Nf = X_CP[idx_Nf_cell, :]\n","Y_Nf = Y_CP[idx_Nf_cell, :]\n","Z_Nf = Z_CP[idx_Nf_cell, :]\n","time_Nf = time_CP[idx_Nf_cell, :]\n","Temp_Nf = Temp_CP[idx_Nf_cell, :]\n","total_heat_Nf = total_heat_CP[idx_Nf_cell, :]\n","\n","# train包含三部分：cell(IC/BC)+cell(CP)+tab,用以在构建loss function时使用不同的cp和rho\n","X_train = torch.vstack((X_Nu_cell, X_Nf, X_Nu_tab)).float()  # Collocation Points of x (CP)\n","Y_train = torch.vstack((Y_Nu_cell, Y_Nf, Y_Nu_tab)).float()\n","Z_train = torch.vstack((Z_Nu_cell, Z_Nf, Z_Nu_tab)).float()\n","time_train = torch.vstack((time_Nu_cell, time_Nf, time_Nu_tab)).float()\n","total_heat_train = torch.vstack((total_heat_Nu_cell, total_heat_Nf, total_heat_Nu_tab)).float()\n","Temp_train = torch.vstack((Temp_Nu_cell, Temp_Nf, Temp_Nu_tab)).float()"]},{"cell_type":"code","source":["from neuromancer.dataset import DictDataset\n","\n","# turn on gradients for PINN\n","X_train.requires_grad = True\n","Y_train.requires_grad = True\n","Z_train.requires_grad = True\n","time_train.requires_grad = True\n","Temp_train.requires_grad = True\n","\n","# Training dataset\n","train_data = DictDataset({'t': time_train, 'y': Y_train, 'x': X_train, 'z': Z_train}, name='train')\n","# test dataset\n","test_data = DictDataset({'t': time_test, 'y': Y_test, 'x': X_test, 'z': Z_test,\n","                         'temperature': Temp_test}, name='test')\n","\n","# torch dataloaders\n","batch_size = X_train.shape[0]  # full batch training\n","train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size,\n","                                           collate_fn=train_data.collate_fn,\n","                                           shuffle=False)\n","test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size,\n","                                          collate_fn=test_data.collate_fn,\n","                                          shuffle=False)\n","\n","from neuromancer.modules import blocks\n","from neuromancer.system import Node\n","\n","# neural net to solve the PDE problem bounded in the PDE domain\n","net = blocks.MLP(insize=4, outsize=1, hsizes=[64, 64], nonlin=nn.Tanh)\n","\n","# symbolic wrapper of the neural net\n","pde_net = Node(net, ['t', 'y', 'x', 'z'], ['temperature_hat'], name='net')\n","\n","print(\"symbolic inputs  of the pde_net:\", pde_net.input_keys)\n","print(\"symbolic outputs of the pde_net:\", pde_net.output_keys)\n","\n","# evaluate forward pass on the train data\n","net_out = pde_net(train_data.datadict)\n","net_out['temperature_hat'].shape"],"metadata":{"id":"xEs6DvHpy3BW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from neuromancer.constraint import variable\n","\n","# symbolic Neuromancer variables\n","temperature_hat = variable('temperature_hat')  # PDE solution generated as the output of a neural net (pde_net)\n","var_t = variable('t')  # temporal domain\n","var_y = variable('y')\n","var_x = variable('x')  # spatial domain\n","var_z = variable('z')  # spatial domain\n","\n","# get the symbolic derivatives\n","dtemperature_dt = (temperature_hat).grad(var_t)\n","dtemperature_dx = (temperature_hat).grad(var_x)\n","dtemperature_dy = (temperature_hat).grad(var_y)\n","dtemperature_dz = (temperature_hat).grad(var_z)\n","d2temperatur_d2x = dtemperature_dx.grad(var_x)\n","d2temperatur_d2y = dtemperature_dy.grad(var_y)\n","d2temperatur_d2z = dtemperature_dz.grad(var_z)\n","# parameters\n","rho_cell = 2092\n","C_p_cell = 678\n","k_cell = 18.2\n","rho_tab = 8978\n","C_p_tab = 381\n","k_tab = 387.6\n","# 创建相应的 rho、C_p、k 张量\n","rho = torch.ones((X_train.shape[0], 1)) * rho_cell\n","C_p = torch.ones((X_train.shape[0], 1)) * C_p_cell\n","k = torch.ones((X_train.shape[0], 1)) * k_cell\n","\n","rho[(Nu_cell + Nf_cell):] = rho_tab\n","C_p[(Nu_cell + Nf_cell):] = C_p_tab\n","k[(Nu_cell + Nf_cell):] = k_tab\n","\n","f_pinn = (rho * C_p * dtemperature_dt - k * d2temperatur_d2x - k * d2temperatur_d2y - k * d2temperatur_d2z\n","          - total_heat_train)\n","\n","# check the shapes of the forward pass of the symbolic PINN terms\n","print(dtemperature_dt({**net_out, **train_data.datadict}).shape)\n","print(dtemperature_dx({**net_out, **train_data.datadict}).shape)\n","print(dtemperature_dy({**net_out, **train_data.datadict}).shape)\n","print(dtemperature_dz({**net_out, **train_data.datadict}).shape)\n","print(d2temperatur_d2x({**net_out, **train_data.datadict}).shape)\n","print(d2temperatur_d2y({**net_out, **train_data.datadict}).shape)\n","print(d2temperatur_d2z({**net_out, **train_data.datadict}).shape)\n","print(f_pinn({**net_out, **train_data.datadict}).shape)\n","\n","# computational graph of the PINN neural network\n","f_pinn.show()\n","# scaling factor for better convergence\n","scaling = 100.\n","\n","# PDE CP loss\n","ell_f = scaling * (f_pinn == 0.) ^ 2\n","\n","# PDE IC and BC loss\n","ell_u1 = scaling * ((temperature_hat[:Nu_cell] == Temp_train[:Nu_cell]) ^ 2)\n","ell_u2 = scaling * ((temperature_hat[-Nu_tab:] == Temp_train[-Nu_tab:]) ^ 2)\n","\n","# # output constraints to bound the PINN solution in the PDE output domain [-1.0, 1.0]\n","\n","con_1 = scaling * (temperature_hat <= min_max(Temp_test)[1]) ^ 2\n","con_2 = scaling * (temperature_hat >= min_max(Temp_test)[0]) ^ 2\n","\n","from neuromancer.loss import PenaltyLoss\n","from neuromancer.problem import Problem\n","\n","# create Neuromancer optimization loss\n","pinn_loss = PenaltyLoss(objectives=[ell_f, ell_u1, ell_u2], constraints=[con_1, con_2])\n","\n","# construct the PINN optimization problem\n","problem = Problem(nodes=[pde_net],  # list of nodes (neural nets) to be optimized\n","                  loss=pinn_loss,  # physics-informed loss function\n","                  grad_inference=True  # argument for allowing computation of gradients at the inference time)\n","                  )"],"metadata":{"id":"DATCLIPKynKh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from neuromancer.trainer import Trainer\n","\n","optimizer = torch.optim.Adam(problem.parameters(), lr=0.01)\n","epochs = 10000\n","\n","#  Neuromancer trainer\n","trainer = Trainer(\n","    problem.to(device),\n","    train_loader,\n","    optimizer=optimizer,\n","    epochs=epochs,\n","    epoch_verbose=200,\n","    train_metric='train_loss',\n","    dev_metric='train_loss',\n","    eval_metric=\"train_loss\",\n","    warmup=epochs,\n",")\n","\n","# Train PINN\n","best_model = trainer.train()\n","\n","# load best trained model\n","problem.load_state_dict(best_model)\n","\n","# evaluate trained PINN on test data\n","PINN = problem.nodes[0]\n","temperature1 = PINN(test_data.datadict)['temperature_hat']\n","\n","\n","# arrange data for plotting\n","temperature_pinn = temperature1.reshape(shape=[time_steps, 388*4]).detach().cpu()\n","\n","# contour(temperature_pinn[89, :], 1)\n","#\n","# contour(temperature_pinn[89, :]-temp[89, :], 1)\n","\n","print(\"0\")"],"metadata":{"id":"KJDkB4aPynTE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"t9ahBnhnynZW"},"execution_count":null,"outputs":[]}]}