{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "# plotting imports\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import contourpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# z=1,2,3,4，表示z方向的4层节点\n",
    "def contour(temp_c, z):\n",
    "    x = np.linspace(-0.0725, 0.0725, 17)\n",
    "    y = np.linspace(-0.096, 0.141, 24)\n",
    "    x_mesh, y_mesh = np.meshgrid(x, y)\n",
    "    temperature = np.zeros_like(x_mesh, dtype=float)\n",
    "    temp_c = temp_c[(z-1)*388:z*388]\n",
    "    index_c = 0\n",
    "    for i_c in range(24):\n",
    "        if i_c < 20:\n",
    "            for j_c in range(17):\n",
    "                temperature[i_c, j_c] = temp_c[index_c]\n",
    "                index_c += 1\n",
    "        if i_c > 19:\n",
    "            for j_c in range(17):\n",
    "                if j_c in {0, 7, 8, 9, 16}:\n",
    "                    temperature[i_c, j_c] = np.nan\n",
    "                else:\n",
    "                    temperature[i_c, j_c] = temp_c[index_c]\n",
    "                    index_c += 1\n",
    "    plt.figure(figsize=(14.5, 23.7))\n",
    "    plt.contourf(x, y, temperature, levels=1000, cmap='jet')\n",
    "    plt.colorbar(label='T')\n",
    "    plt.grid(True, linestyle='--', alpha=0.5)\n",
    "    plt.xlabel('x')\n",
    "    plt.ylabel('y')\n",
    "    plt.title('T')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def file_path(path):\n",
    "    file_paths = []\n",
    "    file_num = 0\n",
    "    for root, dirs, files in os.walk(path):\n",
    "        for file in files:\n",
    "            file_name, file_extension = os.path.splitext(file)\n",
    "            if not file_extension:\n",
    "                file_path = os.path.join(root, file)\n",
    "                file_paths.append(file_path)\n",
    "                file_num += 1\n",
    "    return file_paths, file_num\n",
    "\n",
    "def min_max(data):\n",
    "    min_val = torch.min(data)\n",
    "    max_val = torch.max(data)\n",
    "    return min_val, max_val, max_val-min_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set default dtype to float32\n",
    "torch.set_default_dtype(torch.float)\n",
    "# PyTorch random number generator\n",
    "torch.manual_seed(1234)\n",
    "# Random number generators in other libraries\n",
    "np.random.seed(1234)\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_paths, file_num = file_path(r\"./Data/data20231208\")\n",
    "# 初始化从文件中提取的数据\n",
    "file_x = torch.empty(0)\n",
    "file_y = torch.empty(0)\n",
    "file_z = torch.empty(0)\n",
    "file_temp = torch.empty(0)\n",
    "file_total_heat = torch.empty(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 遍历列表，逐个处理文件\n",
    "for file_path in file_paths:\n",
    "    with open(file_path, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "        data = lines[1:]\n",
    "        # 初始化空列表来存储处理后的数据\n",
    "        processed_data = []\n",
    "\n",
    "        for line in data:\n",
    "            parts = line.strip().split()\n",
    "            processed_line = [float(parts[1]), float(parts[2]), float(parts[3]), float(parts[4]), float(parts[5])]\n",
    "            processed_data.append(processed_line)\n",
    "        tensor_data = torch.tensor(processed_data)\n",
    "        file_x = torch.cat((file_x, tensor_data[:, 0]), dim=0)\n",
    "        file_y = torch.cat((file_y, tensor_data[:, 1]), dim=0)\n",
    "        file_z = torch.cat((file_z, tensor_data[:, 2]), dim=0)\n",
    "        file_temp = torch.cat((file_temp, tensor_data[:, 3]), dim=0)\n",
    "        file_total_heat = torch.cat((file_total_heat, tensor_data[:, 4]), dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 构建张量\n",
    "nodes_num = len(data)\n",
    "nodes = torch.linspace(1, nodes_num, nodes_num)\n",
    "t = torch.linspace(0, 30 * (file_num-1), file_num)\n",
    "T, N = torch.meshgrid(t, nodes, indexing='ij')\n",
    "X = torch.empty_like(N)\n",
    "Y = torch.empty_like(N)\n",
    "Z = torch.empty_like(N)\n",
    "temp = torch.empty_like(N)\n",
    "total_heat = torch.empty_like(N)\n",
    "\n",
    "# 使用循环填入张量\n",
    "index = 0  # 初始化索引\n",
    "for i in range(file_num):\n",
    "    for j in range(nodes_num):\n",
    "        X[i, j] = file_x[index]\n",
    "        Y[i, j] = file_y[index]\n",
    "        Z[i, j] = file_z[index]\n",
    "        temp[i, j] = file_temp[index]\n",
    "        total_heat[i, j] = file_total_heat[index]\n",
    "        index += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contour(temp[89, :], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T = T.to(device)\n",
    "X = X.to(device)\n",
    "Y = Y.to(device)\n",
    "Z = Z.to(device)\n",
    "temp = temp.to(device)\n",
    "total_heat = total_heat.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testdata\n",
    "T_test = T.flatten()[:, None].float()  # the input dataset of t\n",
    "X_test = X.flatten()[:, None].float()  # the input dataset of x\n",
    "Y_test = Y.flatten()[:, None].float()  # the input dataset of y\n",
    "Z_test = Z.flatten()[:, None].float()  # the input dataset of y\n",
    "temp_test = temp.flatten()[:, None].float()\n",
    "total_heat_test = total_heat.flatten()[:, None].float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# boundary condition\n",
    "bottom_x_index = np.array([], dtype=int)\n",
    "bottom_y_index = np.array([], dtype=int)\n",
    "bottom_z_index = np.array([], dtype=int)\n",
    "top_x_index = np.array([], dtype=int)\n",
    "top_y_index = np.array([], dtype=int)\n",
    "top_z_index = np.array([], dtype=int)\n",
    "tab_index = np.array([], dtype=int)\n",
    "for i in range(4):\n",
    "    bottom_x_index = np.append(bottom_x_index, np.arange(i * 388, i * 388 + 20 * 17, 17))\n",
    "    bottom_y_index = np.append(bottom_y_index, np.arange(i * 388, i * 388 + 17, 1))\n",
    "    top_x_index = np.append(top_x_index, np.arange(i * 388 + 16, i * 388 + 20 * 17, 17))\n",
    "    top_y_index = np.append(top_y_index, np.arange(i * 388 + 323, i * 388 + 340, 1))\n",
    "    tab_index = np.append(tab_index, np.arange(i * 388 + 340, i * 388 + 388, 1))\n",
    "bottom_z_index = np.arange(0, 388, 1)\n",
    "top_z_index = np.arange(3*388, 4*388, 1)\n",
    "\n",
    "BC_index = np.concatenate((bottom_x_index, bottom_y_index,bottom_z_index,top_x_index,top_y_index,top_z_index,tab_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initial condition\n",
    "initial_X = X[0, :].reshape(-1, 1)\n",
    "initial_Y = Y[0, :].reshape(-1, 1)\n",
    "initial_T = T[0, :].reshape(-1, 1)\n",
    "initial_Z = Z[0, :].reshape(-1, 1)\n",
    "initial_temp = temp[0, :].reshape(-1, 1)\n",
    "initial_total_heat = total_heat[0, :].reshape(-1, 1)\n",
    "\n",
    "BC_X = X[:, BC_index].reshape(-1, 1)\n",
    "BC_Y = Y[:, BC_index].reshape(-1, 1)\n",
    "BC_T = T[:, BC_index].reshape(-1, 1)\n",
    "BC_Z = Z[:, BC_index].reshape(-1, 1)\n",
    "BC_temp = temp[:, BC_index].reshape(-1, 1)\n",
    "BC_total_heat = total_heat[:, BC_index].reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train data\n",
    "X_train = torch.vstack([initial_X, BC_X])\n",
    "T_train = torch.vstack([initial_T, BC_T])\n",
    "Y_train = torch.vstack([initial_Y, BC_Y])\n",
    "Z_train = torch.vstack([initial_Z, BC_Z])\n",
    "total_heat_train = torch.vstack([initial_total_heat, BC_total_heat])\n",
    "temp_train = torch.vstack([initial_temp, BC_temp])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Nu = int(X_train.shape[0]/ 50) # 1/50 of IC and BC are used in training\n",
    "idx_Nu = np.sort(np.random.choice(X_train.shape[0], Nu, replace=False))\n",
    "X_train_Nu = X_train[idx_Nu, :].float()  # Training Points  of x at (IC+BC)\n",
    "T_train_Nu = T_train[idx_Nu, :].float()  # Training Points  of t at (IC+BC)\n",
    "Y_train_Nu = Y_train[idx_Nu, :].float()  # Training Points  of y at (IC+BC)\n",
    "Z_train_Nu = Z_train[idx_Nu, :].float()  # Training Points  of z at (IC+BC)\n",
    "total_heat_train_Nu = total_heat_train[idx_Nu, :].float()  # Training Points  of y at (IC+BC)\n",
    "temp_train_Nu = temp_train[idx_Nu, :].float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Choose (Nf) Collocation Points\n",
    "Nf = 5000  # Nf: Number of collocation points\n",
    "idx_Nf = np.sort(np.random.choice(nodes_num*file_num, Nf, replace=False))\n",
    "X_train_CP = X_test[idx_Nf, :].view(-1, 1)\n",
    "Y_train_CP = Y_test[idx_Nf, :].view(-1, 1)\n",
    "T_train_CP = T_test[idx_Nf, :].view(-1, 1)\n",
    "Z_train_CP = Z_test[idx_Nf, :].view(-1, 1)\n",
    "temp_train_CP = temp_test[idx_Nf, :].view(-1, 1)\n",
    "total_heat_train_CP = total_heat_test[idx_Nf, :].view(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add IC+BC to the collocation points\n",
    "X_train_Nf = torch.vstack((X_train_CP, X_train_Nu)).float()  # Collocation Points of x (CP)\n",
    "T_train_Nf = torch.vstack((T_train_CP, T_train_Nu)).float()  # Collocation Points of t (CP)\n",
    "Y_train_Nf = torch.vstack((Y_train_CP, Y_train_Nu)).float()\n",
    "Z_train_Nf = torch.vstack((Z_train_CP, Z_train_Nu)).float()\n",
    "total_heat_train_Nf = torch.vstack((total_heat_train_CP, total_heat_train_Nu)).float()\n",
    "temp_train_Nf = torch.vstack((temp_train_CP, temp_train_Nu)).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neuromancer.dataset import DictDataset\n",
    "\n",
    "# turn on gradients for PINN\n",
    "X_train_Nf.requires_grad = True\n",
    "Y_train_Nf.requires_grad = True\n",
    "Z_train_Nf.requires_grad = True\n",
    "T_train_Nf.requires_grad = True\n",
    "temp_train_Nf.requires_grad = True\n",
    "\n",
    "# Training dataset\n",
    "train_data = DictDataset({'t': T_train_Nf, 'y': Y_train_Nf, \n",
    "                          'x': X_train_Nf, 'z': Z_train_Nf}, name='train')\n",
    "# test dataset\n",
    "test_data = DictDataset({'t': T_test, 'y': Y_test, \n",
    "                         'x': X_test, 'z': Z_test,\n",
    "                         'temperature': temp_test}, name='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch dataloaders\n",
    "batch_size = int(X_train_Nf.shape[0])  # full batch training\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size,\n",
    "                                           collate_fn=train_data.collate_fn,\n",
    "                                           shuffle=False)\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size,\n",
    "                                          collate_fn=test_data.collate_fn,\n",
    "                                          shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neuromancer.modules import blocks\n",
    "from neuromancer.system import Node\n",
    "from neuromancer.modules.activations import soft_exp, SoftExponential, SmoothedReLU\n",
    "from model import Model\n",
    "# neural net to solve the PDE problem bounded in the PDE domain\n",
    "# net = blocks.MLPDropout(insize=4, outsize=1, hsizes=[32, 32, 16, 8], nonlin=SoftExponential, dropout=0.2).to(device)\n",
    "\n",
    "net = Model(insize=4, outsize=1, hsizes=[32, 32, 16, 8], nonlin=nn.Tanh, nonlinparam={}, dropout=0.2, )\n",
    "\n",
    "# symbolic wrapper of the neural net\n",
    "pde_net = Node(net, ['t', 'y', 'x', 'z'], ['temperature_hat'], name='net').to(device)\n",
    "\n",
    "print(\"symbolic inputs  of the pde_net:\", pde_net.input_keys)\n",
    "print(\"symbolic outputs of the pde_net:\", pde_net.output_keys)\n",
    "\n",
    "# evaluate forward pass on the train data\n",
    "net_out = pde_net(train_data.datadict)\n",
    "net_out['temperature_hat'].shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neuromancer.constraint import variable\n",
    "\n",
    "# symbolic Neuromancer variables\n",
    "temperature_hat = variable('temperature_hat')  # PDE solution generated as the output of a neural net (pde_net)\n",
    "var_t = variable('t')  # temporal domain\n",
    "var_y = variable('y')\n",
    "var_x = variable('x')  # spatial domain\n",
    "var_z = variable('z')  # spatial domain\n",
    "\n",
    "# get the symbolic derivatives\n",
    "dtemperature_dt = (temperature_hat).grad(var_t)\n",
    "dtemperature_dx = (temperature_hat).grad(var_x)\n",
    "dtemperature_dy = (temperature_hat).grad(var_y)\n",
    "dtemperature_dz = (temperature_hat).grad(var_z)\n",
    "d2temperatur_d2x = dtemperature_dx.grad(var_x)\n",
    "d2temperatur_d2y = dtemperature_dy.grad(var_y)\n",
    "d2temperatur_d2z = dtemperature_dz.grad(var_z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters\n",
    "rho = 2092\n",
    "C_p = 678\n",
    "k = 18.2\n",
    "\n",
    "\n",
    "f_pinn = (rho * C_p * dtemperature_dt - k * d2temperatur_d2x - k * d2temperatur_d2y - k * d2temperatur_d2z\n",
    "          - total_heat_train_Nf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the shapes of the forward pass of the symbolic PINN terms\n",
    "print(dtemperature_dt({**net_out, **train_data.datadict}).shape)\n",
    "print(dtemperature_dx({**net_out, **train_data.datadict}).shape)\n",
    "print(dtemperature_dy({**net_out, **train_data.datadict}).shape)\n",
    "print(dtemperature_dz({**net_out, **train_data.datadict}).shape)\n",
    "print(d2temperatur_d2x({**net_out, **train_data.datadict}).shape)\n",
    "print(d2temperatur_d2y({**net_out, **train_data.datadict}).shape)\n",
    "print(d2temperatur_d2z({**net_out, **train_data.datadict}).shape)\n",
    "print(f_pinn({**net_out, **train_data.datadict}).shape)\n",
    "\n",
    "# computational graph of the PINN neural network\n",
    "f_pinn.show()\n",
    "# scaling factor for better convergence\n",
    "scaling = 100.\n",
    "\n",
    "# PDE CP loss\n",
    "ell_f = (scaling * (f_pinn == torch.tensor(0.).to(device)) ^ 2).to(device)\n",
    "\n",
    "# PDE IC and BC loss\n",
    "# ell_u = (scaling * (temperature_hat[-Nu:] == temp_train_Nu) ^ 2).to(device)  # remember we stacked CP with IC and BC\n",
    "ell_u = (scaling * (temperature_hat == temp_train_Nf) ^ 2).to(device)\n",
    "\n",
    "# # output constraints to bound the PINN solution in the PDE output domain [-1.0, 1.0]\n",
    "\n",
    "con_1 = (scaling * (temperature_hat <= min_max(temp_test)[1]) ^ 2).to(device)\n",
    "con_2 = (scaling * (temperature_hat >= min_max(temp_test)[0]) ^ 2).to(device)\n",
    "\n",
    "from neuromancer.loss import PenaltyLoss\n",
    "from neuromancer.problem import Problem\n",
    "\n",
    "# create Neuromancer optimization loss\n",
    "pinn_loss = PenaltyLoss(objectives=[ell_f, ell_u], constraints=[con_1, con_2])\n",
    "\n",
    "# construct the PINN optimization problem\n",
    "problem = Problem(nodes=[pde_net],  # list of nodes (neural nets) to be optimized\n",
    "                  loss=pinn_loss,  # physics-informed loss function\n",
    "                  grad_inference=True  # argument for allowing computation of gradients at the inference time)\n",
    "                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neuromancer.trainer import Trainer\n",
    "\n",
    "optimizer = torch.optim.Adam(problem.parameters(), lr=1, weight_decay=0.95)\n",
    "epochs = 1000\n",
    "\n",
    "#  Neuromancer trainer\n",
    "trainer = Trainer(\n",
    "    problem.to(device),\n",
    "    train_loader,\n",
    "    optimizer=optimizer,\n",
    "    epochs=epochs,\n",
    "    epoch_verbose=10,\n",
    "    train_metric='train_loss',\n",
    "    dev_metric='train_loss',\n",
    "    eval_metric=\"train_loss\",\n",
    "    warmup=epochs,\n",
    "    device=device,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train PINN\n",
    "best_model = trainer.train()\n",
    "\n",
    "# load best trained model\n",
    "problem.load_state_dict(best_model)\n",
    "\n",
    "# evaluate trained PINN on test data\n",
    "PINN = problem.nodes[0]\n",
    "temperature1 = PINN(test_data.datadict)[\"temperature_hat\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PINN = problem.nodes[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "F.mse_loss(\n",
    "    PINN(test_data.datadict)[\"temperature_hat\"], test_data.datadict['temperature']\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thermal",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
